{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a310864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5d413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16,000 train samples, 2,000 validation samples and 2,000 test samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label_description  label\n",
       "0  im feeling rather rotten so im not very ambiti...           sadness      0\n",
       "1          im updating my blog because i feel shitty           sadness      0\n",
       "2  i never make her separate from me because i do...           sadness      0\n",
       "3  i left with my bouquet of red and yellow tulip...               joy      1\n",
       "4    i was feeling a little vain when i did this one           sadness      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "\n",
    "# Extract the DataFrames\n",
    "train = pd.read_csv('train.csv', delimiter=';')\n",
    "train['label'] = train['label_description'].map({label: i for i, label in enumerate(labels)})\n",
    "\n",
    "val = pd.read_csv('val.csv', delimiter=';')\n",
    "val['label'] = val['label_description'].map({label: i for i, label in enumerate(labels)})\n",
    "\n",
    "test = pd.read_csv('test.csv', delimiter=';')\n",
    "test['label'] = test['label_description'].map({label: i for i, label in enumerate(labels)})\n",
    "    \n",
    "print(f'Loaded {len(train):,} train samples, {len(val):,} validation samples and {len(test):,} test samples.')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "369a5718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00665323,  0.00854131,  0.00724919, -0.00223264,  0.00482249,\n",
       "       -0.00306156, -0.00369475,  0.00128755, -0.00010604,  0.00781875,\n",
       "       -0.00451267, -0.00236597, -0.00560054,  0.00181707, -0.00884478,\n",
       "        0.00453514, -0.00466111,  0.00815185,  0.00152302,  0.00194126,\n",
       "        0.00612788, -0.00019193, -0.00678271, -0.00600964,  0.00153245,\n",
       "        0.00805468,  0.00631805,  0.0028777 , -0.00890056, -0.00811832,\n",
       "        0.00629267,  0.00459466, -0.00502497, -0.00138311,  0.00128885,\n",
       "       -0.00920435, -0.00092909,  0.00592631, -0.0020238 ,  0.00522415,\n",
       "        0.00463007,  0.00577351,  0.00379759,  0.00147334, -0.00101058,\n",
       "        0.00967818, -0.00058655,  0.00477082, -0.00327153, -0.00760753,\n",
       "        0.00574748, -0.00324057, -0.0046496 , -0.00991903,  0.00228867,\n",
       "       -0.00611513,  0.00263402,  0.00010916,  0.00763065,  0.00171987,\n",
       "        0.00270949, -0.00473063,  0.00780364,  0.00924661,  0.00286369,\n",
       "       -0.00265587,  0.00505278,  0.00070206,  0.00039433, -0.00623088,\n",
       "       -0.00536967, -0.0024153 ,  0.00269554,  0.00523233,  0.0072685 ,\n",
       "        0.00812575,  0.00794589,  0.00943225,  0.00416558,  0.00845423,\n",
       "       -0.00192771,  0.0010936 ,  0.00073576,  0.00877375, -0.00926838,\n",
       "       -0.00711263, -0.00233106, -0.00217741,  0.00911532, -0.00309183,\n",
       "        0.00741156, -0.0027856 , -0.00891445,  0.00764242, -0.00607584,\n",
       "       -0.00174677,  0.00931418, -0.00621215, -0.00421908,  0.0077391 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_word2vec(corpus):\n",
    "    '''Fit Word2Vec vectors on a corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list of str):\n",
    "            The corpus to fit the embeddings on.\n",
    "            \n",
    "    Returns:\n",
    "        Word2Vec:\n",
    "            The object containing the fitted embeddings.\n",
    "    '''\n",
    "    # Tokenise the corpus\n",
    "    corpus = [re.split(r' |(?=[\\.\\,\\-\\\"\\'\\!])', doc) for doc in corpus]\n",
    "    \n",
    "    # Fit the embeddings\n",
    "    word2vec = Word2Vec(sentences=corpus, sg=1, workers=-1)\n",
    "\n",
    "    # Return the object containing the fitted embeddings\n",
    "    return word2vec\n",
    "\n",
    "\n",
    "word2vec = fit_word2vec(train.text)\n",
    "word2vec.wv['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a892dd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00370106, -0.00683988,  0.00919236, -0.00453693, -0.0072382 ,\n",
       "       -0.00157117,  0.00474488,  0.00060411,  0.00406659,  0.00976933,\n",
       "       -0.00401155,  0.00908427,  0.00087159,  0.00553001,  0.00729116,\n",
       "        0.00309779,  0.008886  , -0.0090503 , -0.00106582,  0.00544355,\n",
       "        0.00938782,  0.00696341,  0.00021974, -0.00021558,  0.00823147,\n",
       "        0.00375668,  0.00520053, -0.00592092, -0.00020426, -0.00908623,\n",
       "       -0.0010275 , -0.00879216, -0.00116161,  0.00899828, -0.00754742,\n",
       "        0.00814291, -0.00663539,  0.00587001,  0.00436277, -0.00695457,\n",
       "       -0.0077118 , -0.00545783, -0.0088209 , -0.00540636,  0.00124033,\n",
       "       -0.00850474, -0.00982233,  0.00746922,  0.00382913, -0.00382764,\n",
       "        0.0042289 , -0.00193098,  0.00914407,  0.0028104 ,  0.00404169,\n",
       "       -0.00942   , -0.00812456, -0.00245567,  0.0063874 , -0.00817601,\n",
       "        0.00257316, -0.00077865, -0.0037706 ,  0.00490162, -0.00452152,\n",
       "        0.00812759, -0.00820326,  0.00282187, -0.00199904, -0.00682571,\n",
       "        0.00243992, -0.0068015 ,  0.00521613, -0.00815318, -0.00743087,\n",
       "       -0.00594012,  0.00305079, -0.00059137, -0.00328922,  0.00578743,\n",
       "       -0.00834642,  0.00257241, -0.00836627, -0.00172216,  0.00276546,\n",
       "       -0.00646565,  0.00162152,  0.00833919,  0.00181987, -0.003423  ,\n",
       "       -0.00464878, -0.005332  ,  0.00919311, -0.00315445,  0.00420455,\n",
       "        0.00961181,  0.00639404, -0.00694526,  0.008623  ,  0.00278347],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word2vec_embed_term(term):\n",
    "    '''Embeds a term using the fitted Word2vec model.\n",
    "    \n",
    "    Args:\n",
    "        term (str):\n",
    "            The term to be embedded.\n",
    "            \n",
    "    Returns:\n",
    "        NumPy array:\n",
    "            The embedding of the term.\n",
    "    '''\n",
    "    # We do a `try-except` here to deal with out-of-vocabulary terms,\n",
    "    try:\n",
    "        embedding = word2vec.wv[term]\n",
    "        \n",
    "    # If the word is not in our vocabulary, then we just embed it as the zero vector\n",
    "    except KeyError:\n",
    "        embedding = np.zeros(100)\n",
    "    \n",
    "    # Return the embedding\n",
    "    return embedding\n",
    "\n",
    "word2vec_embed_term('sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c3f5b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.5711163e-04,  4.5147138e-03,  3.4552026e-03,  1.7170995e-03,\n",
       "       -2.4658772e-03,  1.5286887e-03, -1.5400040e-03,  1.2311387e-03,\n",
       "        1.2559954e-04,  1.7002800e-03, -4.2510824e-04, -1.8027722e-04,\n",
       "        4.7017343e-04, -9.4091170e-04, -3.7611471e-03, -3.8669235e-03,\n",
       "       -2.2440739e-03,  3.8658944e-04, -2.3945253e-03,  1.0323343e-03,\n",
       "       -1.4382165e-03, -8.6477457e-04, -2.3490316e-03,  2.9262356e-03,\n",
       "       -1.2347619e-03,  1.2851731e-03,  2.0692684e-03, -1.3876378e-03,\n",
       "       -1.9872310e-03, -9.7388460e-04,  4.2622131e-03, -4.4900523e-03,\n",
       "       -7.0473459e-04, -3.6706894e-03,  4.0059979e-04, -1.6730311e-03,\n",
       "        1.2301751e-03,  2.9771584e-03,  7.1987446e-04, -3.4074578e-04,\n",
       "        3.2544734e-03, -6.2678452e-04, -2.3427657e-03,  1.4261820e-03,\n",
       "       -2.9835878e-03, -2.2870649e-03, -1.7279612e-03,  1.0918805e-03,\n",
       "       -2.3612874e-03, -1.2422171e-03,  5.3027896e-03, -3.0977703e-03,\n",
       "       -1.5055535e-03, -2.5990722e-04, -4.6229786e-03, -1.4673702e-03,\n",
       "        4.1652932e-03, -2.7581921e-03, -3.6462937e-03, -4.3593729e-03,\n",
       "        2.0074483e-03,  2.3741145e-03,  3.8615889e-03,  8.3705178e-05,\n",
       "       -8.5805182e-04,  2.0515749e-03,  3.2376475e-03,  1.0163957e-03,\n",
       "       -1.2244240e-03, -4.4244854e-03,  1.3995362e-03,  2.4568560e-03,\n",
       "        9.6885063e-04,  1.5532258e-03,  3.5421068e-03,  5.0779055e-03,\n",
       "        5.6056096e-04,  5.7036635e-03,  1.1531649e-03,  5.1738909e-03,\n",
       "       -1.8430563e-03, -2.4615633e-03,  2.7490992e-03, -7.5640692e-04,\n",
       "       -5.9247543e-03, -1.3156354e-03, -4.4421665e-04,  5.7543960e-04,\n",
       "       -4.7179800e-04,  3.1352839e-03,  4.9057016e-03,  4.7735131e-04,\n",
       "       -4.1127126e-03,  2.5971315e-03,  1.0704496e-03, -2.3241012e-04,\n",
       "        1.2526024e-03, -1.8775631e-03, -4.1257483e-03,  5.3395296e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word2vec_embed_doc(doc, aggregation_fn=np.mean):\n",
    "    '''Embeds a document using the fitted Word2vec model.\n",
    "    \n",
    "    Args:\n",
    "        doc (str):\n",
    "            The document to be embedded.\n",
    "        aggregation_fn (callable, optional):\n",
    "            The function used to aggregate the term embeddings\n",
    "            in the document. Must be a NumPy function. Defaults to \n",
    "            `numpy.mean`, meaning that the average of the term \n",
    "            embeddings is returned.\n",
    "            \n",
    "    Returns:\n",
    "        NumPy array:\n",
    "            The embedding of the document.\n",
    "    '''\n",
    "    # Split up the document into a list of terms\n",
    "    terms = re.split(r' |(?=[\\.\\,\\-\\\"\\'\\!])', doc)\n",
    "    \n",
    "    # Embed each term using the fitted `word2vec` model.\n",
    "    embeddings = [word2vec_embed_term(term) for term in terms]\n",
    "    \n",
    "    # Aggregate the embeddings according to `aggregation_fn`\n",
    "    embedding = aggregation_fn(embeddings, axis=0)\n",
    "    \n",
    "    # Return the aggregated embedding\n",
    "    return embedding\n",
    "\n",
    "word2vec_embed_doc('this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e74d5acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model achieved a 16.67% balanced accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "def word2vec_train_model(train_dataset: pd.DataFrame,\n",
    "                         val_dataset: pd.DataFrame) -> LogisticRegression:\n",
    "    '''Trains a logistic regression model on a Word2vec embedded corpus.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset (Pandas DataFrame):\n",
    "            The dataset on which to train the logistic regression \n",
    "            model. Must have a 'text' and 'label' column.\n",
    "        val_dataset (Pandas DataFrame):\n",
    "            The dataset on which to evaluate the logistic regression \n",
    "            model. Must have a 'text' and 'label' column.\n",
    "            \n",
    "    Returns:\n",
    "        LogisticRegression:\n",
    "            The trained model.\n",
    "    '''    \n",
    "    # Embded datasets\n",
    "    word2vec_embeddings_train = [word2vec_embed_doc(doc) for doc in train_dataset.text]\n",
    "    word2vec_embeddings_val = [word2vec_embed_doc(doc) for doc in val_dataset.text]\n",
    "        \n",
    "    # Convert the lists of embeddings to one big matrix\n",
    "    word2vec_embeddings_train = np.stack(word2vec_embeddings_train)\n",
    "    word2vec_embeddings_val = np.stack(word2vec_embeddings_val)\n",
    "    \n",
    "    # Define the classification model\n",
    "    model = LogisticRegression(max_iter=1_000)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(word2vec_embeddings_train, train_dataset.label)\n",
    "    \n",
    "    # Evaluate the model on the validation set.\n",
    "    predictions = model.predict(word2vec_embeddings_val)\n",
    "    val_balanced_acc = balanced_accuracy_score(val_dataset.label, predictions)\n",
    "    print(f'The model achieved a {100 * val_balanced_acc:.2f}% '\n",
    "          f'balanced accuracy on the validation set.')\n",
    "    \n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n",
    "model = word2vec_train_model(train_dataset=train, val_dataset=val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
